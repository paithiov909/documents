---
ID: 258876b77a47c7783250
Title: Rで言語処理100本ノックを解くわけがない（前半）
Tags: R,自然言語処理,NLP,言語処理100本ノック
Author: Kato Akiru
Private: false
---

## この記事について

以下のブログ記事で「言語処理100本ノック」にRで取り組んでいます。このQiita記事はその一部をコピペしてきたものです。

- [Rで言語処理100本ノックを解くわけがない（１）](https://wabbitspunch.netlify.app/2020/05/01/100-knocks-2020-1/)
- [Rで言語処理100本ノックを解くわけがない（２）](https://wabbitspunch.netlify.app/2020/05/01/100-knocks-2020-2/)
- [Rで言語処理100本ノックを解くわけがない（３）](https://wabbitspunch.netlify.app/2020/05/01/100-knocks-2020-3/)

解いているだけなので解説とかはしていません。また、力不足でふつうにつらいところなどを端折っています。続きは気が向いたらやります。

## 全体の見通し

2020年版に触ってみますが、ぜんぶは解きません。無理です。

- [言語処理100本ノック 2020](https://nlp100.github.io/ja/)

ググって出てくる範囲では2015年版にはyamano357さんがRで取り組んでいます。RcppでMeCabとCaboChaのバインディングを自分で書いて解いている本格派です。

- [Rによる言語処理100本ノック前半まとめ - バイアスと戯れる](http://yamano357.hatenadiary.com/entry/2015/07/27/001728)
- [Rによる言語処理100本ノック後半まとめと全体での総括 - バイアスと戯れる](http://yamano357.hatenadiary.com/entry/2015/10/22/193839)

2020年版もRでやろうとしている人がいるようです。

- [言語処理100本ノック R - Qiita](https://qiita.com/PiyoMoasa/items/7c1a6cca3f9cbcaf7773)

2020年版も7章の単語ベクトルあたりまではPure Rでいけそうですが、おそらく8章のディープ・ニューラルネットあたりからバックエンドにPythonを利用することになり、10章の最終題の翻訳デモの構築でふつうにPythonを利用しなければならなくなるはずなので詰みます。

## 準備運動

### 00. 文字列の逆順

```r
stringr::str_split("stressed", pattern = "") %>%
  purrr::map(~ rev(.)) %>%
  unlist() %>%
  stringr::str_c(collapse = "")
#> [1] "desserts"
```

### 01. 「パタトクカシーー」

```r
stringr::str_split("パタトクカシーー", pattern = "") %>%
  purrr::map(~ purrr::pluck(.[c(TRUE, FALSE)])) %>%
  unlist() %>%
  stringr::str_c(collapse = "")
#> [1] "パトカー"
```

### 02. 「パトカー」＋「タクシー」＝「パタトクカシーー」

```r
list("パトカー", "タクシー") %>%
  purrr::map(~ stringr::str_split(., pattern = "")) %>%
  purrr::flatten() %>%
  purrr::pmap(~ stringr::str_c(.x, .y, collapse = "")) %>%
  unlist() %>%
  stringr::str_c(collapse = "")
#> [1] "パタトクカシーー"
```

### 03. 円周率

```r
stringr::str_split("Now I need a drink, alcoholic of course, after the heavy lectures involving quantum mechanics.", pattern = " ") %>%
  purrr::flatten() %>%
  purrr::map(~ stringr::str_count(., pattern = "[:alpha:]")) %>%
  unlist()
#>  [1] 3 1 4 1 5 9 2 6 5 3 5 8 9 7 9
```

### 04. 元素記号

```r
stringr::str_split("Hi He Lied Because Boron Could Not Oxidize Fluorine. New Nations Might Also Sign Peace Security Clause. Arthur King Can.", pattern = " ") %>%
  purrr::flatten() %>%
  purrr::imap(~
  dplyr::if_else(
    .y %in% c(1, 5, 6, 7, 8, 9, 15, 16, 19),
    stringr::str_sub(.x, 1, 1),
    stringr::str_sub(.x, 1, 2)
  )) %>%
  purrr::imap(function(x, i) {
    names(x) <- i
    return(x)
  }) %>%
  unlist()
#>    1    2    3    4    5    6    7    8    9   10   11   12   13   14   15 
#>  "H" "He" "Li" "Be"  "B"  "C"  "N"  "O"  "F" "Ne" "Na" "Mi" "Al" "Si"  "P" 
#>   16   17   18   19   20 
#>  "S" "Cl" "Ar"  "K" "Ca"
```

### 05. n-gram

```r
ngram <- function(x, n = 2, sep = " ") {
  stopifnot(is.character(x))
  #### 先例がみんな`embed`を使っているが、ここでは使わない ####

  tokens <- unlist(stringr::str_split(x, pattern = sep))
  len <- length(tokens)

  if (len < n) {
    res <- character(0)
  } else {
    res <- sapply(1:max(1, len - n + 1), function(i) {
      stringr::str_c(tokens[i:min(len, i + n - 1)], collapse = " ")
    })
  }

  return(res)
}
ngram("I am an NLPer")
#> [1] "I am"     "am an"    "an NLPer"
```

### 08. 暗号文

```r
cipher <- function(str) {
  f <- purrr::as_mapper(~ 219 - .)
  v <- stringr::str_split(str, pattern = "", simplify = TRUE)
  res <- sapply(v[1, ], function(char) {
    dplyr::if_else(
      stringr::str_detect(char, "[:lower:]"),
      char %>%
        charToRaw() %>%
        as.integer() %>%
        f() %>%
        as.raw() %>%
        rawToChar(),
      char
    )
  })
  return(stringr::str_c(res, collapse = ""))
}
cipher("I couldn't believe that I could actually understand what I was reading : the phenomenal power of the human mind.")
#> [1] "I xlfowm'g yvorvev gszg I xlfow zxgfzoob fmwvihgzmw dszg I dzh ivzwrmt : gsv
```

### 09. Typoglycemia

```r
typoglycemia <- function(str) {
  f <- function(char) {
    subset <- stringr::str_sub(char, 2, nchar(char) - 1) %>%
      stringr::str_split(pattern = "") %>%
      purrr::flatten() %>%
      sample()
    res <- stringr::str_c(
      c(
        stringr::str_sub(char, 1, 1),
        subset,
        stringr::str_sub(char, nchar(char), nchar(char))
      ),
      collapse = ""
    )
    return(res)
  }
  res <- stringr::str_split(str, pattern = " ") %>%
    purrr::flatten() %>%
    purrr::map(~
    dplyr::if_else(
      nchar(stringr::str_subset(., "[:alpha:]|:")) <= 4,
      .,
      f(.)
    ))
  return(stringr::str_c(res, collapse = " "))
}
typoglycemia("I couldn't believe that I could actually understand what I was reading : the phenomenal power of the human mind.")
#> [1] "I cnod'ult bvileee that I cluod aclaltuy uenrdantsd what I was reading : the pan
```

## UNIXコマンド

まるごと割愛します

## 正規表現

### 20. JSONデータの読み込み

```r
temp <- tempfile(fileext = ".gz")
download.file("https://nlp100.github.io/data/jawiki-country.json.gz", temp)
con <- gzfile(description = temp, open = "rb", encoding = "UTF-8")
jsonfile <- readr::read_lines(con) %>%
  purrr::map_dfr(~
  jsonlite::fromJSON(.))
close(con)

jsonfile %>%
  dplyr::filter(title == "イギリス") %>%
  dplyr::pull(text) %>%
  glimpse() ## 長いので
#>  chr "{{redirect|UK}}\n{{redirect|英国|春秋時代の諸侯国|英 (春秋)}}\n{{Otheruses|ヨーロッパ
```

### 21. カテゴリ名を含む行を抽出

```r
lines <- jsonfile %>%
  dplyr::filter(title == "イギリス") %>%
  dplyr::pull(text) %>%
  readr::read_lines() %>%
  stringr::str_subset(stringr::fixed("[[Category:"))
lines
#> [1] "[[Category:イギリス|*]]"                
#> [2] "[[Category:イギリス連邦加盟国]]"        
#> [3] "[[Category:英連邦王国|*]]"              
#> [4] "[[Category:G8加盟国]]"                  
#> [5] "[[Category:欧州連合加盟国|元]]"         
#> [6] "[[Category:海洋国家]]"                  
#> [7] "[[Category:現存する君主国]]"            
#> [8] "[[Category:島国]]"                      
#> [9] "[[Category:1801年に成立した国家・領域]]"
```

## 形態素解析

気をきかせて{readtext}で読みこんでおきます。

```r
temp <- tempfile(fileext = ".txt")
download.file("https://nlp100.github.io/data/neko.txt", temp)
neko <- readtext::readtext(temp, encoding = "UTF-8")
neko$text[1] %>%
  readr::read_lines(skip_empty_rows = TRUE) %>%
  length()
#> [1] 9210
```

### 30. 形態素解析結果の読み込み

{RMeCab}はtaggerをラップしていて必要な情報を取りづらいので、[オレオレパッケージ](https://paithiov909.github.io/rjavacmecab/)を使います。動くはずです（ただし、{RMeCab}などに比べて5~10倍程度は遅い）。{RcppMeCab}でもできますが、公式のリポジトリのソースはWindows環境だとビルドにコケるのでUNIX系の環境が必要です（2020年5月現在）。

データフレームで持ちたいのですが、すべて変換すると死ぬほど時間がかかるのでここでは一部の解析結果だけ使います。

```r
neko_txt_mecab <- rjavacmecab::cmecab_c(neko$text[1])[1:900] %>%
  purrr::map_dfr(function(line) {
    row <- stringr::str_split(line, " ", simplify = TRUE)
    attr <- rjavacmecab::tokenize(row[1, 2]) %>%
      t() %>%
      as.data.frame(stringsAsFactors = FALSE)
    res <- row[1, 1] %>%
      as.data.frame(stringsAsFactors = FALSE) %>%
      dplyr::bind_cols(attr)
    return(res)
  })
colnames(neko_txt_mecab) <- c(
  "Surface",
  "POS1",
  "POS2",
  "POS3",
  "POS4",
  "X5StageUse1",
  "X5StageUse2",
  "Base",
  "Reading",
  "Pronunciation"
)
head(neko_txt_mecab)
#>   Surface   POS1   POS2 POS3 POS4 X5StageUse1 X5StageUse2 Base  Reading
#> 1      一   名詞     数    *    *           *           *   一     イチ
#> 2      　   記号   空白    *    *           *           *   　       　
#> 3    吾輩   名詞 代名詞 一般    *           *           * 吾輩 ワガハイ
#> 4      は   助詞 係助詞    *    *           *           *   は       ハ
#> 5      猫   名詞   一般    *    *           *           *   猫     ネコ
#> 6      で 助動詞      *    *    *    特殊・ダ      連用形   だ       デ
#>   Pronunciation
#> 1          イチ
#> 2            　
#> 3      ワガハイ
#> 4            ワ
#> 5          ネコ
#> 6            デ
```

### 33. 「AのB」

```r
neko_txt_mecab %>%
  tibble::rowid_to_column() %>%
  dplyr::filter(Surface == "の") %>%
  dplyr::pull(rowid) %>%
  purrr::keep(~ neko_txt_mecab$POS1[. - 1] == "名詞" && neko_txt_mecab$POS1[. + 1] == "名詞") %>%
  purrr::map_chr(~ stringr::str_c(
    neko_txt_mecab$Surface[. - 1],
    neko_txt_mecab$Surface[.],
    neko_txt_mecab$Surface[. + 1],
    collapse = ""
  ))
#>  [1] "彼の掌"     "掌の上"     "書生の顔"   "はずの顔"   "顔の真中"  
#>  [6] "穴の中"     "書生の掌"   "掌の裏"     "何の事"     "肝心の母親"
#> [11] "藁の上"     "笹原の中"   "池の前"     "池の上"     "一樹の蔭"  
#> [16] "垣根の穴"   "隣家の三"   "時の通路"   "一刻の猶予" "家の内"    
#> [21] "彼の書生"   "以外の人間" "前の書生"
```

### 34. 名詞の連接

これたぶんまちがっています。`purrr::map`で書いていますが、こういうことするならふつうにforループで書かないとダメな気がする。

```r
idx <- neko_txt_mecab %>%
  tibble::rowid_to_column() %>%
  dplyr::filter(POS1 == "名詞") %>%
  dplyr::pull(rowid) %>%
  purrr::discard(~ neko_txt_mecab$POS1[. + 1] != "名詞")

search_in <- idx

purrr::map_chr(search_in, function(idx) {
  itr <- idx
  res <- stringr::str_c(neko_txt_mecab$Surface[idx])
  while (neko_txt_mecab$POS1[itr + 1] == "名詞") {
    res <- stringr::str_c(res, neko_txt_mecab$Surface[itr + 1])
    search_in <<- purrr::discard(search_in, ~ . == itr + 1)
    itr <- itr + 1
  }
  return(res)
})
#>  [1] "人間中"       "一番獰悪"     "時妙"         "一毛"        
#>  [5] "その後猫"     "一度"         "ぷうぷうと煙" "邸内"        
#>  [9] "三毛"         "書生以外"
```

### 37. 「猫」と共起頻度の高い上位10語

解釈のしかたが複数あるけれど、はじめに形態素解析する段階で文区切りを無視してしまったので、ここではbi-gramを数えてお茶をにごします。

```r
neko_txt_mecab %>%
  tibble::rowid_to_column() %>%
  dplyr::filter(Surface == "猫") %>%
  dplyr::mutate(Colocation = stringr::str_c(Surface, neko_txt_mecab$Surface[rowid + 1], sep = " - ")) %>%
  dplyr::group_by(Colocation) %>%
  dplyr::count(Colocation, sort = TRUE) %>%
  head(10) %>%
  ggplot(aes(x = reorder(Colocation, -n), y = n)) +
  geom_col() +
  labs(x = "Colocation", y = "Freq") +
  theme_light()
```

  ![neko_colocation](https://wabbitspunch.netlify.app/post/2020-05-01-100-knocks-2020-2_files/figure-html/unnamed-chunk-9-1.png)

### 39. Zipfの法則

~~ずっと雰囲気で書いている ggplot2なんもわからん~~

```r
count <- neko_txt_mecab %>%
  dplyr::group_by(Base) %>%
  dplyr::count(Base) %>%
  dplyr::ungroup()
count %>%
  tibble::rowid_to_column() %>%
  dplyr::mutate(rank = nrow(count) + 1 - dplyr::min_rank(count$n)[rowid]) %>%
  ggplot(aes(x = rank, y = n)) +
  geom_point() +
  labs(x = "Rank of Freq", y = "Freq") +
  scale_x_log10() +
  scale_y_log10() +
  theme_light()
```

![zipf](https://wabbitspunch.netlify.app/post/2020-05-01-100-knocks-2020-2_files/figure-html/unnamed-chunk-11-1.png)

## 係り受け解析

### 40. 係り受け解析結果の読み込み（形態素）

ここでも[オレオレパッケージ](https://paithiov909.github.io/pipian/)を使います。設問の通りにクラスを実装したりはしませんが、だいたい似たような情報を出力できます。

ただ、以下の処理は律儀に文の数だけCaboChaを呼んでいるため、非常に遅いです（ちゃんとやる場合、`pipian::cabochaFlatXML`に文章をまるごと渡して、戻り値のflat XMLを自分で解析したほうがよいでしょう）。ここでは、解析するのはごく一部だけにしています。

```r
res <- neko$text[1] %>%
  readr::read_lines(skip_empty_rows = TRUE)
res <- stringr::str_c(res[1:20], sep = " ") %>%
  iconv(from = "UTF-8", to = "CP932") %>%
  purrr::imap_dfr(function(str, sid) {
    res <- pipian::cabochaFlatXML(str) %>%
      pipian::CabochaR()
    df <- res$as.tibble()
    return(
      dplyr::bind_cols(
        data.frame(sid = rep(sid, nrow(df))),
        df
      )
    )
  })

head(res)
#>   sid chunk_id D1 D2 rel     score head func tok_id word   POS1   POS2
#> 1   1        3  0 -1   D  0.000000    0    0      0   一   名詞     数
#> 2   2        3  0  2   D -0.764522    0    0      0   　   記号   空白
#> 3   2        5  1  2   D -0.764522    1    2      1 吾輩   名詞 代名詞
#> 4   2        5  1  2   D -0.764522    1    2      2   は   助詞 係助詞
#> 5   2        8  2 -1   D  0.000000    3    5      3   猫   名詞   一般
#> 6   2        8  2 -1   D  0.000000    3    5      4   で 助動詞      *
#>   POS3 POS4 X5StageUse1 X5StageUse2 Original    Yomi1    Yomi2
#> 1    *    *           *           *       一     イチ     イチ
#> 2    *    *           *           *       　       　       　
#> 3 一般    *           *           *     吾輩 ワガハイ ワガハイ
#> 4    *    *           *           *       は       ハ       ワ
#> 5    *    *           *           *       猫     ネコ     ネコ
#> 6    *    *    特殊・ダ      連用形       だ       デ       デ
```

3文目の形態素列

```r
res %>%
  dplyr::filter(sid == 3) %>%
  dplyr::select(word)
#>   word
#> 1 名前
#> 2   は
#> 3 まだ
#> 4 無い
#> 5   。
```

### 42. 係り元と係り先の文節の表示

```r
memo <- res %>%
  dplyr::filter(POS1 != "記号") %>%
  dplyr::group_by(sid, chunk_id) %>%
  dplyr::mutate(
    chunk = stringr::str_c(
      word,
      collapse = ""
    )
  ) %>%
  dplyr::ungroup() %>%
  dplyr::select(sid, chunk_id, D1, D2, chunk) %>%
  dplyr::distinct()

memo %>%
  dplyr::filter(D2 != -1) %>%
  dplyr::group_by(sid, chunk_id, D1) %>%
  dplyr::mutate(colocation = stringr::str_c(
    chunk,
    memo$chunk[memo$sid == .data$sid & memo$D1 == .data$D2],
    sep = " "
  )) %>%
  dplyr::ungroup() %>%
  dplyr::select(chunk, colocation) %>%
  head()
#> # A tibble: 6 x 2
#>   chunk    colocation     
#>   <chr>    <chr>          
#> 1 吾輩は   吾輩は 猫である
#> 2 名前は   名前は 無い    
#> 3 まだ     まだ 無い      
#> 4 どこで   どこで 生れたか
#> 5 生れたか 生れたか つかぬ
#> 6 とんと   とんと つかぬ
```

### 43. 名詞を含む文節が動詞を含む文節に係るものを抽出

```r
memo <- res %>%
  dplyr::group_by(sid, chunk_id) %>%
  dplyr::mutate(
    chunk = stringr::str_c(
      word,
      collapse = ""
    )
  ) %>%
  dplyr::ungroup() %>%
  dplyr::mutate(tag = POS1 == "動詞") %>%
  dplyr::select(sid, chunk_id, D1, D2, chunk, POS1, tag) %>%
  dplyr::distinct()

memo %>%
  dplyr::filter(POS1 == "名詞") %>%
  dplyr::filter(D2 != -1) %>%
  dplyr::group_by(sid, chunk_id, D1) %>%
  dplyr::mutate(colocation = stringr::str_c(
    chunk,
    memo$chunk[memo$sid == .data$sid & memo$D1 == .data$D2 & memo$tag == TRUE],
    sep = " "
  )) %>%
  dplyr::ungroup() %>%
  dplyr::select(chunk, colocation) %>%
  dplyr::filter(chunk != colocation) %>%
  head()
#> # A tibble: 6 x 2
#>   chunk        colocation                 
#>   <chr>        <chr>                      
#> 1 　どこで     　どこで 生れたか          
#> 2 見当が       見当が つかぬ。            
#> 3 所で         所で 泣いて                
#> 4 ニャーニャー ニャーニャー 泣いて        
#> 5 いた事だけは いた事だけは 記憶している。
#> 6 吾輩は       吾輩は 見た。
```

### 44. 係り受け木の可視化

そういう関数があるのでサボります。

```r
graph <- neko$text[1] %>%
  readr::read_lines(skip_empty_rows = TRUE)
tbl <- graph[12] %>%
  iconv(from = "UTF-8", to = "CP932") %>%
  pipian::CabochaTbl()
tbl$plot()
```

![dependency](https://wabbitspunch.netlify.app/post/2020-05-01-100-knocks-2020-3_files/figure-html/unnamed-chunk-6-1.png)

### 45. 動詞の格パターンの抽出

```r
memo <- res %>%
  dplyr::select(sid, chunk_id, D1, D2, POS1, Original)

pattern <- memo %>%
  dplyr::filter(POS1 == "動詞") %>%
  dplyr::group_by(sid, chunk_id, D1) %>%
  dplyr::mutate(colocation = stringr::str_c(
    "",
    memo$Original[memo$sid == .data$sid & memo$D2 == .data$D1 & memo$POS1 == "助詞"],
    collapse = " "
  )) %>%
  dplyr::ungroup() %>%
  dplyr::select(Original, colocation)

pattern
#> # A tibble: 49 x 2
#>    Original colocation
#>    <chr>    <chr>     
#>  1 生れる   で        
#>  2 つく     か が     
#>  3 する     ""        
#>  4 泣く     で        
#>  5 する     て だけ は
#>  6 いる     て だけ は
#>  7 始める   で        
#>  8 見る     は を     
#>  9 聞く     で        
#> 10 捕える   を        
#> # ... with 39 more rows
```

「する」「見る」「与える」という動詞の格パターン

```r
pattern %>%
  dplyr::filter(Original %in% c("する", "見る", "与える")) %>%
  dplyr::group_by(Original, colocation) %>%
  dplyr::count()
#> # A tibble: 9 x 3
#> # Groups:   Original, colocation [9]
#>   Original colocation     n
#>   <chr>    <chr>      <int>
#> 1 する     ""             4
#> 2 する     が             2
#> 3 する     が と で       1
#> 4 する     て だけ は     1
#> 5 する     も             1
#> 6 する     をもって       1
#> 7 見る     て を          1
#> 8 見る     の             1
#> 9 見る     は を          1
```

### 46. 動詞の格フレーム情報の抽出

これを見ると「だけは」みたいな助詞の連続が要件通りに表示できていないことがわかりますが、もう疲れたのであきらめます。

```r
memo <- res %>%
  dplyr::group_by(sid, chunk_id) %>%
  dplyr::mutate(
    chunk = stringr::str_c(
      word,
      collapse = ""
    )
  ) %>%
  dplyr::ungroup() %>%
  dplyr::select(sid, chunk_id, D1, D2, POS1, Original, chunk) %>%
  dplyr::distinct()

pattern <- memo %>%
  dplyr::filter(POS1 == "動詞") %>%
  dplyr::group_by(sid, chunk_id, D1) %>%
  dplyr::mutate(colocation = stringr::str_c(
    "",
    memo$Original[memo$sid == .data$sid & memo$D2 == .data$D1 & memo$POS1 == "助詞"],
    collapse = " "
  )) %>%
  dplyr::mutate(chunk = stringr::str_c(
    "",
    memo$chunk[memo$sid == .data$sid & memo$D2 == .data$D1 & memo$POS1 == "助詞"],
    collapse = " "
  )) %>%
  dplyr::ungroup() %>%
  dplyr::select(Original, colocation, chunk)

pattern
#> # A tibble: 49 x 3
#>    Original colocation chunk                           
#>    <chr>    <chr>      <chr>                           
#>  1 生れる   で         　どこで                        
#>  2 つく     か が      生れたか 見当が                 
#>  3 する     ""         ""                              
#>  4 泣く     で         所で                            
#>  5 する     て だけ は 泣いて いた事だけは いた事だけは
#>  6 いる     て だけ は 泣いて いた事だけは いた事だけは
#>  7 始める   で         ここで                          
#>  8 見る     は を      吾輩は ものを                   
#>  9 聞く     で         あとで                          
#> 10 捕える   を         我々を                          
#> # ... with 39 more rows
```
