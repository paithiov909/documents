---
ID: a47a097836e8a9ec12ef
Title: R（quanteda）によるテキスト解析
Tags: R,自然言語処理,NLP,テキストマイニング
Author: Kato Akiru
Private: false
---

## この記事について

ブログ記事のコピペです。

## Googleドキュメントの読み込み

これまで自分が書いてきた文章について、テキスト解析をしてみます。ここで分析している文章はnoteで読むことができます。

> [さちこ｜note](https://note.com/shinabitanori)

これまでに書いた文章はいくつかの場所にバックアップを取っていて、Googleドキュメントにもバックアップがあります。今回はそれらを{googledrive}で取得し、{readtext}で読み込みます。

`googledrive::drive_download`はディレクトリごとダウンロードしたりはできないようなので、特定のディレクトリにあるファイルのリスト（dribble）を`dplyr::rowwise`で行ごとに渡して`dplyr::do`のなかでダウンロードしています（`dplyr::rowwise() %>% dplyr::do()`という流れで処理するやり方は手元にあるdplyr v0.8.0の時点ですでにquestioningなので、将来的に使えなくなる可能性があります）。

```{r}
aquarium <- googledrive::drive_ls("Documents/aquarium/") %>%
  dplyr::rowwise() %>%
  dplyr::do(googledrive::drive_download(
    googledrive::as_id(.$id),
    path = file.path(tempdir(), .$name),
    overwrite = TRUE,
    verbose = FALSE
  ))
shinabitanori <- googledrive::drive_ls("Documents/shinabitanori/") %>%
  dplyr::rowwise() %>%
  dplyr::do(googledrive::drive_download(
    googledrive::as_id(.$id),
    path = file.path(tempdir(), .$name),
    overwrite = TRUE,
    verbose = FALSE
  ))
```

ダウンロードしたdocxファイルのリストをデータフレームとして持っておきます。文章は公開されている場所などに応じて2つのディレクトリに分けて保存されています。ここでは、この保存されているディレクトリを文書の変数として持つようにします。

```{r}
df <- list("aquarium", "shinabitanori") %>%
  purrr::map_dfr(~
  dplyr::mutate(rlang::eval_tidy(rlang::parse_expr(.)), doc = .)) %>%
  dplyr::select(doc, name, local_path) %>%
  tibble::rowid_to_column()

df[1, ]
#> Source: local data frame [1 x 4]
#> Groups: <by row>
#> 
#> # A tibble: 1 x 4
#>   rowid doc     name       local_path                                      
#>   <int> <chr>   <chr>      <chr>                                           
#> 1     1 aquari~ nd8e25452~ "C:\\Users\\user\\AppData\\Local\\Temp\\RtmpOk0~
```

## 形態素解析

[この自作パッケージ](https://github.com/paithiov909/tangela)を使っています。このパッケージは[atilika/kuromoji](https://github.com/atilika/kuromoji)をrJava経由で呼べるようにしたもので、形態素解析をおこなう関数`tangela::kuromoji`のみを提供するシンプルなものです。文字列はUTF-8として渡す前提で、戻り値のテキストのエンコーディングもUTF-8になります。

結果を{quanteda}のコーパスオブジェクトとして格納して、いろいろ試していきます。

```{r}
normalize <- function(str) {
  str %>%
    stringr::str_replace_all("\u2019", "\'") %>%
    stringr::str_replace_all("\u201d", "\"") %>%
    stringr::str_replace_all("[\u02d7\u058a\u2010\u2011\u2012\u2013\u2043\u207b\u208b\u2212]", "-") %>%
    stringr::str_replace_all("[\ufe63\uff0d\uff70\u2014\u2015\u2500\u2501\u30fc]", enc2utf8("\u30fc")) %>%
    stringr::str_replace_all("[~\u223c\u223e\u301c\u3030\uff5e]", "~") %>%
    stringr::str_remove_all("[:punct:]") %>%
    stringr::str_remove_all("[:blank:]") %>%
    stringr::str_remove_all("[:cntrl:]") %>%
    return()
}

corp <- df %>%
  dplyr::rowwise() %>%
  dplyr::do(readtext::readtext(.$local_path, docvarsfrom = "filenames", docvarnames = c("name"))) %>%
  dplyr::bind_rows() %>%
  dplyr::right_join(
    dplyr::select(df, rowid, doc, name),
    by = "name"
  ) %>%
  dplyr::mutate(text = normalize(text)) %>%
  dplyr::mutate(tokens = stringr::str_c(
    purrr::map_chr(
      tangela::kuromoji(text), ~
      purrr::pluck(., "surface")
    ),
    collapse = " "
  )) %>%
  quanteda::corpus(text_field = "tokens")
```

## ワードクラウド

ストップワードとして`rtweet::stopwordslangs`を利用しています。

```{r}
stopwords <- rtweet::stopwordslangs %>%
  dplyr::filter(lang == "ja") %>%
  dplyr::filter(p >= .98) %>%
  dplyr::pull(word)

corp %>%
  quanteda::tokens(what = "fastestword") %>%
  quanteda::tokens_remove(stopwords, valuetype = "fixed") %>%
  quanteda::dfm(groups = "doc") %>%
  quanteda::dfm_trim(min_termfreq = 3L) %>%
  quanteda::textplot_wordcloud(comparison = TRUE, color = viridis::cividis(3))
```

![wordcloud](https://rawcdn.githack.com/paithiov909/wabbitspunch/6c7e23967f9838fb0df953315535f730b52ed53d/static/posts/shinabitanori-google-docs_files/figure-html/unnamed-chunk-4-1.png)

## 出現頻度の集計

```{r}
corp %>%
  quanteda::tokens(what = "fastestword") %>%
  quanteda::tokens_remove(stopwords, valuetype = "fixed") %>%
  quanteda::dfm() %>%
  quanteda::dfm_weight("prop") %>%
  quanteda::textstat_frequency(groups = "doc") %>%
  dplyr::top_n(-16L, rank) %>%
  ggpubr::ggdotchart(
    x = "feature",
    y = "frequency",
    group = "group",
    color = "group",
    rotate = TRUE
  ) +
  theme_bw()
```

![freqstats](https://rawcdn.githack.com/paithiov909/wabbitspunch/6c7e23967f9838fb0df953315535f730b52ed53d/static/posts/shinabitanori-google-docs_files/figure-html/unnamed-chunk-5-1.png)

## Keyness

aquariumグループの文書とその他の対照を見ています。

```{r}
corp %>%
  quanteda::tokens(what = "fastestword") %>%
  quanteda::tokens_remove(stopwords, valuetype = "fixed") %>%
  quanteda::dfm(groups = "doc") %>%
  quanteda::textstat_keyness(target = "lyrical") %>%
  quanteda::textplot_keyness()
```

![keyness](https://rawcdn.githack.com/paithiov909/wabbitspunch/6c7e23967f9838fb0df953315535f730b52ed53d/static/posts/shinabitanori-google-docs_files/figure-html/unnamed-chunk-6-1.png)

## 対応分析

```{r}
corp %>%
  quanteda::tokens(what = "fastestword") %>%
  quanteda::tokens_remove(stopwords, valuetype = "fixed") %>%
  quanteda::dfm() %>%
  quanteda::dfm_weight(scheme = "prop") %>%
  quanteda.textmodels::textmodel_ca() %>%
  quanteda.textmodels::textplot_scale1d(
    margin = "documents",
    groups = quanteda::docvars(corp, "doc")
  )
```

![ca](https://rawcdn.githack.com/paithiov909/wabbitspunch/6c7e23967f9838fb0df953315535f730b52ed53d/static/posts/shinabitanori-google-docs_files/figure-html/unnamed-chunk-7-1.png)

## 共起ネットワーク

```{r}
corp %>%
  quanteda::tokens(what = "fastestword") %>%
  quanteda::tokens_remove(stopwords, valuetype = "fixed") %>%
  quanteda::dfm(groups = "doc") %>%
  quanteda::dfm_trim(min_termfreq = 20L) %>%
  quanteda::fcm() %>%
  quanteda::textplot_network(min_freq = .8)
```

![network](https://rawcdn.githack.com/paithiov909/wabbitspunch/6c7e23967f9838fb0df953315535f730b52ed53d/static/posts/shinabitanori-google-docs_files/figure-html/unnamed-chunk-8-1.png)

## クラスタリング

マンハッタン距離、ward法（ward.D2）です。

```{r}
d <- corp %>%
  quanteda::tokens(what = "fastestword") %>%
  quanteda::tokens_remove(stopwords, valuetype = "fixed") %>%
  quanteda::dfm() %>%
  quanteda::dfm_weight(scheme = "prop") %>%
  quanteda::textstat_dist(method = "manhattan", diag = TRUE) %>%
  as.dist() %>%
  hclust(method = "ward.D2") %>%
  ggdendro::dendro_data(type = "rectangle") %>%
  purrr::list_modify(
    labels = dplyr::bind_cols(
      .$labels,
      names = quanteda::docvars(corp, "name"),
      doc = quanteda::docvars(corp, "doc")
    )
  )

ggplot(ggdendro::segment(d)) +
  geom_segment(aes(x = x, y = y, xend = xend, yend = yend)) +
  geom_text(ggdendro::label(d), mapping = aes(x, y, label = names, colour = doc, hjust = 0), size = 3) +
  coord_flip() +
  scale_y_reverse(expand = c(.2, 0)) +
  ggdendro::theme_dendro()
```

![clustering](https://rawcdn.githack.com/paithiov909/wabbitspunch/6c7e23967f9838fb0df953315535f730b52ed53d/static/posts/shinabitanori-google-docs_files/figure-html/unnamed-chunk-9-1.png)

## LDA（Latent Dirichlet Allocation）

```{r}
dtm <- corp %>%
  quanteda::tokens(what = "fastestword") %>%
  quanteda::tokens_remove(stopwords, valuetype = "fixed") %>%
  quanteda::dfm() %>%
  quanteda::dfm_tfidf()

features <- corp %>%
  quanteda::tokens(what = "fastestword") %>%
  quanteda::tokens_remove(stopwords, valuetype = "fixed") %>%
  quanteda::dfm(groups = "name") %>%
  quanteda::ntoken()

m <- dtm %>%
  as("dgCMatrix") %>%
  textmineR::FitLdaModel(k = 3, iterations = 200, burnin = 175)

textmineR::GetTopTerms(m$phi, 5)
#>      t_1        t_2      t_3       
#> [1,] "グループ" "ゴリラ" "x"       
#> [2,] "条例"     "因子"   "テクスト"
#> [3,] "価値"     "物語"   "ポエジー"
#> [4,] "文節"     "文体"   "感情"    
#> [5,] "批評"     "孤独"   "運動会"
```

{LDAvis}で可視化してみます。ただ、{LDAvis}はもうしばらくメンテナンスされていないパッケージで、ちょっと挙動があやしいところがあります。たとえば、デフォルトロケールがCP932であるWindows環境の場合、`LDAvis::createJSON`で書き出されるラベル（vocab）のエンコーディングがそっちに引きずられてCP932になってしまうため、ブラウザで表示したときにラベルが文字化けします。書き出されたlda.jsonをUTF-8に変換すれば文字化けは解消されるので、とりあえずあとから変換して上書きするとよいです。

```{r}
LDAvis::createJSON(
  phi = m$phi,
  theta = m$theta,
  doc.length = features,
  vocab = stringi::stri_enc_toutf8(dtm @ Dimnames$features),
  term.frequency = quanteda::colSums(dtm)
) %>%
  LDAvis::serVis(open.browser = FALSE, out.dir = file.path(getwd(), "cache"))
#> Warning in dir.create(out.dir): 'C:
#> \Users\user\Documents\GitHub\wabbitspunch\content\post\cache' はすでに存在
#> します
#>  要求されたパッケージ servr をロード中です

readr::read_lines_raw(file.path(getwd(), "cache", "lda.json")) %>%
  iconv(from = "CP932", to = "UTF-8") %>%
  jsonlite::parse_json(simplifyVector = TRUE) %>%
  jsonlite::write_json(file.path(getwd(), "cache", "lda.json"), dataframe = "columns", auto_unbox = TRUE)
```

## GloVe

```{r}
toks <- corp %>%
  quanteda::tokens(what = "fastestword") %>%
  as.list() %>%
  text2vec::itoken()

vocab <- toks %>%
  text2vec::create_vocabulary() %>%
  text2vec::prune_vocabulary(term_count_min = 5L)

vect <- text2vec::vocab_vectorizer(vocab)

tcm <- text2vec::create_tcm(
  it = toks,
  vectorizer = vect,
  skip_grams_window = 5L
)

glove <- text2vec::GlobalVectors$new(
  rank = 50
  x_max = 15L
)

wv <- glove$fit_transform(
  x = tcm,
  n_iter = 10L
) %>%
  as.data.frame(stringsAsFactors = FALSE) %>%
  tibble::as_tibble(.name_repair = "minimal", rownames = NA)
#> INFO [2020-05-20 13:41:50] 2020-05-20 13:41:50 - epoch 1, expected cost 0.0992
#> INFO [2020-05-20 13:41:50] 2020-05-20 13:41:50 - epoch 2, expected cost 0.0485
#> INFO [2020-05-20 13:41:50] 2020-05-20 13:41:50 - epoch 3, expected cost 0.0384
#> INFO [2020-05-20 13:41:50] 2020-05-20 13:41:50 - epoch 4, expected cost 0.0324
#> INFO [2020-05-20 13:41:50] 2020-05-20 13:41:50 - epoch 5, expected cost 0.0281
#> INFO [2020-05-20 13:41:51] 2020-05-20 13:41:51 - epoch 6, expected cost 0.0248
#> INFO [2020-05-20 13:41:51] 2020-05-20 13:41:51 - epoch 7, expected cost 0.0223
#> INFO [2020-05-20 13:41:51] 2020-05-20 13:41:51 - epoch 8, expected cost 0.0201
#> INFO [2020-05-20 13:41:51] 2020-05-20 13:41:51 - epoch 9, expected cost 0.0184
#> INFO [2020-05-20 13:41:51] 2020-05-20 13:41:51 - epoch 10, expected cost 0.0169
```

{Rtsne}で次元を減らして可視化します。

```{r}
getRtsneAsTbl <- function(tbl, dim = 2, perp = 30) {
  tsn <- tbl %>% Rtsne::Rtsne(dim = dim, perplexity = perp)
  tsny <- tsn$Y
  rownames(tsny) <- row.names(tbl)
  tsny <- as.data.frame(tsny, stringsAsFactors = FALSE)
  return(tibble::as_tibble(tsny, .name_repair = "minimal", rownames = NA))
}

vec <- vocab %>%
  dplyr::anti_join(
    y = tibble::tibble(words = stopwords),
    by = c("term" = "words")
  ) %>%
  dplyr::arrange(desc(term_count)) %>%
  head(100) %>%
  dplyr::left_join(tibble::rownames_to_column(wv), by = c("term" = "rowname")) %>%
  tibble::column_to_rownames("term") %>%
  dplyr::select(V1, V2)

dist <- proxy::dist(x = vec, y = vec, method = "Euclidean", diag = TRUE)
clust <- kmeans(x = dist, centers = 5)
vec <- getRtsneAsTbl(vec, perp = 2) %>%
  tibble::rownames_to_column() %>%
  dplyr::mutate(cluster = as.factor(clust$cluster))

vec %>%
  ggplot(aes(x = V1, y = V2, colour = cluster)) +
  ggrepel::geom_text_repel(aes(label = row.names(vec))) +
  theme_light()
```

![glove](https://rawcdn.githack.com/paithiov909/wabbitspunch/6c7e23967f9838fb0df953315535f730b52ed53d/static/posts/shinabitanori-google-docs_files/figure-html/unnamed-chunk-14-1.png)

